[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stanford Health Policy Data Science",
    "section": "",
    "text": "Lab Manual\nThe Health Policy Data Science Lab at Stanford is a group of interdisciplinary researchers who develop and apply quantitative methods to solve problems in health policy, leveraging techniques from statistics, computer science, economics, epidemiology, and decision science. The faculty PI at Stanford is Professor Sherri Rose.\nThis manual will evolve over time to incorporate additional contributions from Lab members and collaborators. Contributors include Malcolm Barrett, Marika Cusick, and Sherri Rose.\nFeel free to draw from this manual (and please cite it if you do)! We drew inspiration from the lab manuals of our colleagues, including Jade Benjamin-Chung and Russ Poldrack.\nThis work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.",
    "crumbs": [
      "Lab Manual"
    ]
  },
  {
    "objectID": "chapters/01-culture.html",
    "href": "chapters/01-culture.html",
    "title": "1  Lab Culture",
    "section": "",
    "text": "1.1 Mutual Respect in the Lab\nWe strive for a culture of mutual respect in all of our communications, meetings, and policies. Please demonstrate respect for all Lab members by, for example, practicing active listening, speaking only for yourself and not others, and not dominating conversations. Mutual respect for each other’s time includes being prepared (including drafting agendas), planning ahead, and starting/ending meetings at the scheduled time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab Culture</span>"
    ]
  },
  {
    "objectID": "chapters/01-culture.html#sec-respect-people-data",
    "href": "chapters/01-culture.html#sec-respect-people-data",
    "title": "1  Lab Culture",
    "section": "1.2 Respect for the People Represented in Study Data",
    "text": "1.2 Respect for the People Represented in Study Data\nA key value in the Lab is deep respect for the people and communities whose information is represented in the data we study. We engage seriously in learning about the policies, institutional structures, societal biases, and lived experiences that underlie these data sources.\nThis respect also involves careful attention to data privacy and protection, discussed in Chapter 4.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab Culture</span>"
    ]
  },
  {
    "objectID": "chapters/01-culture.html#sec-working-hours",
    "href": "chapters/01-culture.html#sec-working-hours",
    "title": "1  Lab Culture",
    "section": "1.3 Working Hours",
    "text": "1.3 Working Hours\nLab members are encouraged to work efficiently and effectively on a schedule that works well for them. We do not support a culture of overwork!\nDr. Rose does not expect you to be working in the evenings or on weekends and asks that you respect Lab members’ evening and weekend time as well. This includes not expecting Dr. Rose to review your work product, submit letters, or otherwise be available for typical work tasks outside business hours.\nWe embrace time off, breaks from meetings, and vacations!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab Culture</span>"
    ]
  },
  {
    "objectID": "chapters/01-culture.html#trainee-support-access-and-accomodations",
    "href": "chapters/01-culture.html#trainee-support-access-and-accomodations",
    "title": "1  Lab Culture",
    "section": "1.4 Trainee Support, Access, and Accomodations",
    "text": "1.4 Trainee Support, Access, and Accomodations\nYour physical and mental health are incredibly important. Please familiarize yourself with the mental health and crisis assistance resources available for students at Stanford as well as mental health resources for postdoctoral scholars.\nStanford is committed to providing equal educational opportunities for disabled students. Disabled students are a valued and essential part of the Stanford community. If you experience disability, please register with the Office of Accessible Education (OAE). Professional staff at OAE will evaluate your needs, support appropriate and reasonable accommodations, and prepare an Academic Accommodation Letter for faculty. If you already have an Academic Accommodation Letter, Stanford invites you to share your letter with your advisor. Academic Accommodation Letters should be shared at the earliest possible opportunity so we may partner with you and OAE to identify any barriers to access and inclusion that might be encountered in your experience.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab Culture</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html",
    "href": "chapters/02-policies.html",
    "title": "2  General Policies",
    "section": "",
    "text": "2.1 Joining the Lab\nDr. Rose keeps a page updated on her website regarding whether she is taking new students, hiring postdocs, or available for dissertation committees. At Stanford, we’ve had students from many different graduate programs join the Lab, including health policy, biomedical data science, computer science, and chemical engineering programs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#recurring-meetings",
    "href": "chapters/02-policies.html#recurring-meetings",
    "title": "2  General Policies",
    "section": "2.2 Recurring Meetings",
    "text": "2.2 Recurring Meetings\nLab trainees meet individually with Dr. Rose regularly. This is typically weekly or every other week depending on the needs of the trainee and their projects. Meetings are either 25 minutes or 50 minutes to allow for breaks between meetings. Dr. Rose expects trainees to manage the meeting such that it ends on time.\nIn general, Dr. Rose expects progress to be made between each meeting. Progress can include struggling with the material. Dr. Rose cares that you are engaged and taking initiative to advance the project. We’ll discuss what you learned and where you are still confused. When we identify areas where you have gaps, Dr. Rose does expect you to invest in learning the required material and to follow through to fill those gaps. All trainees must be making satisfactory academic progress in line with the expectations of their graduate degree program.\nIf you do not need to meet a particular week (e.g., making steady progress and don’t have questions), please email Dr. Rose in advance so she can efficiently reallocate the meeting time. Your physical and mental health are important. If you need to cancel a meeting for health or personal reasons, email Dr. Rose. As much notice as is possible is helpful, but it is always better to cancel a meeting short notice than to attend when you are sick!\nIf there are repeated meeting cancellations, we should discuss the underlying reasons.\nPlease also see Section 2.3 for details on recurring meeting agendas and GitHub repository updates.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#sec-meeting-agendas",
    "href": "chapters/02-policies.html#sec-meeting-agendas",
    "title": "2  General Policies",
    "section": "2.3 Meeting Agendas & GitHub Repository Updates",
    "text": "2.3 Meeting Agendas & GitHub Repository Updates\nTrainees must prepare an agenda prior to each recurring meeting with Dr. Rose. Create a google doc (invite Dr. Rose as editor) that you’ll add to in reverse chronological order for each meeting with the information below included. Update the google doc by 11AM one business day before our meeting. Push a commit to the project’s GitHub repository at the same time, even if you plan to make further changes prior to the meeting. If additional changes are made to the repository ahead of the meeting, push a second commit. Repeatedly not creating agendas will result in cancelled meetings.\n[Meeting Date]\n\nWhat has been completed since previous meeting:\nTopics to discuss at the meeting:\nWhat will be completed by the next meeting:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#individual-development-plans",
    "href": "chapters/02-policies.html#individual-development-plans",
    "title": "2  General Policies",
    "section": "2.4 Individual Development Plans",
    "text": "2.4 Individual Development Plans\nIf Dr. Rose is your primary advisor, students and postdoctoral scholars should complete the Stanford Individual Development Plan (IDP) when joining the Lab and then annually thereafter (student forms, initial form for postdocs, annual form for postdocs). This applies regardless of home department. Trainees should plan to check in on progress made toward IDP goals once a quarter. Dr. Rose expects that trainees will be responsible for scheduling the annual IDP meetings and adding the IDP check-ins once a quarter to the agenda for an existing recurring meeting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#academic-progress-in-graduate-degree-programs",
    "href": "chapters/02-policies.html#academic-progress-in-graduate-degree-programs",
    "title": "2  General Policies",
    "section": "2.5 Academic Progress in Graduate Degree Programs",
    "text": "2.5 Academic Progress in Graduate Degree Programs\nIf you are a graduate student and Dr. Rose is your primary advisor, please create and regularly update a shared document that includes major degree requirements (e.g., coursework, teaching, qualifying exams, etc) and completion status (e.g., completed, planned completion date). Include links at the top to the graduate degree handbook from your home department as well as key contacts in your home department who should be kept apprised of your academic progress. This document will be discussed quarterly along with the IDP check-ins, and students should add it to the agenda for an existing recurring meeting.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#registering-for-units",
    "href": "chapters/02-policies.html#registering-for-units",
    "title": "2  General Policies",
    "section": "2.6 Registering for Units",
    "text": "2.6 Registering for Units\nGraduate students should discuss their plans to register for research units with Dr. Rose each term (often BIOMEDIN 299 or HRP 399). Units should be taken credit/no credit and not for a letter grade. Permission codes are currently required to register for research units.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#deadlines",
    "href": "chapters/02-policies.html#deadlines",
    "title": "2  General Policies",
    "section": "2.7 Deadlines",
    "text": "2.7 Deadlines\nWe aim to set ambitious yet feasible target deadlines for work product in a collaborative process. It is often the case that research takes longer than we expect, and an internal agreed-upon deadline is no longer possible. If you anticipate missing a deadline, contact Dr. Rose. It is an expectation in the Lab that all members are proactive about discussing revised deadlines rather than waiting until after the deadline has passed. If a trainee is repeatedly missing deadlines, we should discuss the underlying reasons.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#lab-meetings-events",
    "href": "chapters/02-policies.html#lab-meetings-events",
    "title": "2  General Policies",
    "section": "2.8 Lab Meetings & Events",
    "text": "2.8 Lab Meetings & Events\nThe Lab holds Lab Meetings and various types of events throughout the year, including lunches, data jamborees, coffee chats, and journal clubs. If you have ideas for events, suggestions are always welcome.\nFood at Lab events is funded by the Lab and free to Lab attendees. We expect that trainees who RSVP and submit food orders will show up to the event, barring illness or personal situation. If you need to change your RSVP, please contact Dr. Rose to help us avoid food waste.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#recommendation-letters",
    "href": "chapters/02-policies.html#recommendation-letters",
    "title": "2  General Policies",
    "section": "2.9 Recommendation Letters",
    "text": "2.9 Recommendation Letters\nDr. Rose receives a variety of requests for letters from current and former trainees, including for graduate school, job applications, fellowships, and awards. Please know she treats your letter requests with the seriousness they deserve and recognizes their importance. Dr. Rose asks that you respect the time it takes her to write, revise, and send letters by giving plenty of notice and being organized with your requests.\nIt essential that you ask Dr. Rose if she can write you a recommendation letter before submitting her name as a recommender. It may be the case that a letter from someone else will be more beneficial for you or that the deadline is too soon for her to accommodate your request.\nIn general, please give Dr. Rose at least 4 weeks notice when requesting a letter, and more time than that when possible. If the deadline is sooner than 4 weeks, she may not be able to write the letter, particularly during times of the year when she receives a high volume of letter requests.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/02-policies.html#communication",
    "href": "chapters/02-policies.html#communication",
    "title": "2  General Policies",
    "section": "2.10 Communication",
    "text": "2.10 Communication\nWe have a Lab slack. Lab members can search for “HPDS Lab” in the Workspaces at Stanford and request to join.\nPlease keep in mind our Lab philosophy on working hours in Section 1.3. Do not assume that because you have sent a slack message or email that you should get an instant reply.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>General Policies</span>"
    ]
  },
  {
    "objectID": "chapters/03-funding.html",
    "href": "chapters/03-funding.html",
    "title": "3  Funding",
    "section": "",
    "text": "3.1 For Graduate Students\nGraduate students may be funded by training grants, research grants, individual fellowships, and other sources.\nInternal Stanford fellowship funding opportunities include:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Funding</span>"
    ]
  },
  {
    "objectID": "chapters/03-funding.html#for-graduate-students",
    "href": "chapters/03-funding.html#for-graduate-students",
    "title": "3  Funding",
    "section": "",
    "text": "Knight-Hennessy Scholars\nDiversifying Academia, Recruiting Excellence (DARE) Doctoral Fellowship\nStanford Graduate Fellowship in Science & Engineering (SGF)\nStanford Interdisciplinary Graduate Fellowship (SIGF)\nStanford Data Science Scholars\nStanford Bio-X Fellows\nResearch, Action, and Impact through Strategic Engagement (RAISE) Doctoral Fellowship.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Funding</span>"
    ]
  },
  {
    "objectID": "chapters/03-funding.html#for-postdoctoral-scholars",
    "href": "chapters/03-funding.html#for-postdoctoral-scholars",
    "title": "3  Funding",
    "section": "3.2 For Postdoctoral Scholars",
    "text": "3.2 For Postdoctoral Scholars\nFunding opportunities for postdoctoral scholars include training grants, research grants, individual fellowships, and other sources.\nInternal Stanford fellowship funding opportunities include:\n\nPropel Postdoctoral Scholars Program\nStanford Science Fellows\n\nAdditional Stanford fellowships can be found at the Office of Postdoctoral Affairs website.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Funding</span>"
    ]
  },
  {
    "objectID": "chapters/03-funding.html#active-lab-funding",
    "href": "chapters/03-funding.html#active-lab-funding",
    "title": "3  Funding",
    "section": "3.3 Active Lab Funding",
    "text": "3.3 Active Lab Funding\nThe Health Policy Data Science Lab at Stanford currently has three active research grants where Dr. Rose is the Principal Investigator:\n\nNIH Director’s Pioneer Award\nNLM R01 Grant\nLaura and John Arnold Foundation Grant\n\nDr. Rose is also Co-Principal Investigator on the following two research grants:\n\nStanford Impact Labs Grant\nStanford HAI Hoffman-Yee Grant",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Funding</span>"
    ]
  },
  {
    "objectID": "chapters/04-data.html",
    "href": "chapters/04-data.html",
    "title": "4  Data",
    "section": "",
    "text": "4.1 Human Subjects Training\nEach team member must complete required trainings on the protection of human subjects when working with study data. Additional trainings may be required depending on the study.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "chapters/04-data.html#computer-encryption-and-security-management",
    "href": "chapters/04-data.html#computer-encryption-and-security-management",
    "title": "4  Data",
    "section": "4.2 Computer Encryption and Security Management",
    "text": "4.2 Computer Encryption and Security Management\nIn order to work with study data, your computer must be encrypted and have security management software. Please confirm your computer is compliant with the most up-to-date requirements listed on Stanford’s websites as these processes can change.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "chapters/04-data.html#irbs",
    "href": "chapters/04-data.html#irbs",
    "title": "4  Data",
    "section": "4.3 IRBs",
    "text": "4.3 IRBs\nInstitutional Review Board (IRB) approvals are required for many of the studies we conduct, which can include studies involving the secondary analysis of de-identified data. Before accessing such data, you will need to either be added to an already approved IRB protocol or we will complete and submit a new IRB protocol for our project.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "chapters/04-data.html#data-at-stanford",
    "href": "chapters/04-data.html#data-at-stanford",
    "title": "4  Data",
    "section": "4.4 Data at Stanford",
    "text": "4.4 Data at Stanford\nThis section includes an abbreviated overview of the data sets stored at Stanford that inform our active research projects in the Lab.\n\n4.4.1 STARR Data\nThe STAnford Research Repository (STARR) contains data from Stanford Health Care. Lab experts on the STARR data include Marika Cusick.\n\n\n4.4.2 Center for Population Health Sciences\nThe Data Core at the Stanford Center for Population Health Sciences (PHS) provides access to a range of health data sources. There are additional training requirements in order to access data at PHS and these must be completed before working with PHS data.\n\nMedicare Data\nLab experts on Medicare data include Marissa Reitsma and Oana Enache.\n\n\nAmerican Family Cohort Registry\nThe American Family Cohort (AFC) Registry of primary care data was created and is updated by the American Board of Family Medicine (ABFM). Lab experts on the ABFM AFC Cohort Registry data include Agata Foryciarz, Gabriela Basel, and Marika Cusick.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "chapters/04-data.html#bringing-data-to-stanford",
    "href": "chapters/04-data.html#bringing-data-to-stanford",
    "title": "4  Data",
    "section": "4.5 Bringing Data to Stanford",
    "text": "4.5 Bringing Data to Stanford\nThe key steps in bringing a new external data set to Stanford include submitting an IRB and a data risk assessment review (DRA). A summary of the DRA review process at Stanford is included below. However, the DRA review process is subject to change and should be confirmed and followed as described on the DRA website.\n\nReview the Stanford Risk Classifications to determine the level of risk of your requested data.\nIf the requested data are high risk, then you will need to submit a DRA. If you are not sure if the data are high risk, there is also a pre-screening form that helps assess whether a DRA form is necessary.\nIn the DRA form, you will need the following information:\n\nProject information\n\nProject leader contact information\nIRB information (if applicable)\nFunding source\nAny other relevant parties involved in the project (e.g., Stanford Health Care)\nAny other individuals who will be involved with the data\n\nWho are you getting the data from? (third party)\n\nContact information (e.g., name and email address)\nData flow diagram\nAre the data going in or out of the U.S.?\n\nBrief description of the project and reason for needing this data source\nBrief description of the data source\n\nElements (e.g., lab results, diagnoses or procedures)\nNumber of records\nData dictionary (if available)\nData source (e.g., institutions and individuals involved in producing the data)\nWhether the data are identified or de-identified and how are the data de-identified? (e.g., Safe Harbor method)\n\n\nAwait the DRA review. You may get follow-up questions from the University Privacy Office, such as:\n\nHow do you plan to store the data?\nWill Stanford data be used or shared?\nWill data be shared back with the third party?\n\n\nLab experts on the DRA process include Marika Cusick.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "chapters/04-data.html#data-sharing",
    "href": "chapters/04-data.html#data-sharing",
    "title": "4  Data",
    "section": "4.6 Data Sharing",
    "text": "4.6 Data Sharing\nMany of our studies involve secondary analyses of existing health databases. It is typically not permitted for us to share such data due to privacy considerations. Thus, we often created simulated data that has some similar properties to the health databases to share along with our code and published results. However, in certain cases, this type of simulated data sharing may not be permitted by the data use agreement.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "chapters/04-data.html#simulated-data",
    "href": "chapters/04-data.html#simulated-data",
    "title": "4  Data",
    "section": "4.7 Simulated Data",
    "text": "4.7 Simulated Data\nMany of our projects involve simulating data to test our methodology under situations where we know the underlying truth and because we cannot share certain health data due to privacy considerations. Simulating data is an important skill to learn.\nExamples of detailed simulation studies designed by Lab alums include work from Irina Degtiar and Anna Zink.\nNote: Creating simulated data of this type is different than designing a microsimulation study.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "chapters/05-computing.html",
    "href": "chapters/05-computing.html",
    "title": "5  Computing Resources",
    "section": "",
    "text": "5.1 Nero GCP\nStanford’s Nero Google Cloud Platform (GCP) is the environment often most suitable or required for the analysis of data sources discussed in Chapter 4. There are costs associated with Nero GCP and we must keep careful track of these.\nIf you have questions about which environment you should use for your data analyses, please ask so we can be certain to follow all data security and privacy requirements.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computing Resources</span>"
    ]
  },
  {
    "objectID": "chapters/05-computing.html#sherlock",
    "href": "chapters/05-computing.html#sherlock",
    "title": "5  Computing Resources",
    "section": "5.2 Sherlock",
    "text": "5.2 Sherlock\nStanford has a High Performance Computing cluster called Sherlock. See this information on getting started, connecting, and submitting jobs.\nThe Lab has a dedicated node on Sherlock. Lab members can view internal documentation on submitting jobs to our node here.\nSherlock is most frequently used for analyses involving simulated data as it is not designed for data with a high risk classification. If you have questions about which environment you should use for your data analyses, please ask so we can be certain to follow all data security and privacy requirements.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computing Resources</span>"
    ]
  },
  {
    "objectID": "chapters/05-computing.html#software",
    "href": "chapters/05-computing.html#software",
    "title": "5  Computing Resources",
    "section": "5.3 Software",
    "text": "5.3 Software\nMuch of the software we use in the Lab is available open source for free. Please let Dr. Rose know if there is non-free software that would be helpful for your research and we can likely purchase this with research funds.\n\nStatistical Computing\n\nR: Download Free\nRStudio: Download Free\nPython: Download Free\n\n\n\nIllustration\n\nOmniGraffle: Purchase Subscription or One-Time Download\nAdobe Illustrator: Free via Stanford Branner Earth Science Library or Purchase Adobe Creative Cloud License",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Computing Resources</span>"
    ]
  },
  {
    "objectID": "chapters/06-conferences.html",
    "href": "chapters/06-conferences.html",
    "title": "6  Conferences",
    "section": "",
    "text": "6.1 Stanford Travel Policies\nFlights for Lab trainees and staff must be purchased through Stanford Travel. They are not reimbursable otherwise. Hotels must be purchased through Stanford Travel unless it is a conference hotel. Please copy Dr. Rose on travel-related emails with administrative staff.\nHealth Policy and Biomedical Data Science Students: If you are presenting at a conference, submit for up to $1,000 of those expenses to be reimbursed through the Biosciences Travel Grant Program. You are eligible for one conference per year.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conferences</span>"
    ]
  },
  {
    "objectID": "chapters/06-conferences.html#sec-conferences-field",
    "href": "chapters/06-conferences.html#sec-conferences-field",
    "title": "6  Conferences",
    "section": "6.2 Conferences by Field",
    "text": "6.2 Conferences by Field\nWe include below a noncomprehensive list of conferences that may be of interest to Lab members.\n\nAlgorithmic Bias & Fairness\n\nACM FAccT\nAAI/ACM AI, Ethics, & Society\nACM EAAMO\nIEEE SaTML\n\n\n\nMachine Learning for Health\n\nMachine Learning for Health Care\nCHIL\nMachine Learning for Health\n\n\n\nStatistics\n\nInternational Conference on Health Policy Statistics\nENAR\nJoint Statistical Meetings\n\n\n\nEpidemiology\n\nSociety for Epidemiologic Research\n\n\n\nHealth Economics\n\nASHEcon\niHEA\nEconomics of AI\n\n\n\nHealth Policy\n\nAcademyHealth\nISPOR\n\n\n\nDecision Science\n\nSMDM\n\n\n\nStatistical Programming\n\nposit::conf()\nSciPy\nuseR!\nPyCon\nNew York R Conference",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Conferences</span>"
    ]
  },
  {
    "objectID": "chapters/07-publications.html",
    "href": "chapters/07-publications.html",
    "title": "7  Publications",
    "section": "",
    "text": "7.1 Authorship\nWe aim to discuss authorship early in the research process and have continuing conversations regarding team member roles. Plans can change and contributions may evolve over time. If you have authorship questions during the process of working on a project, we want you to feel empowered to ask these questions!\nIt is an expectation in the Lab that no new authors are invited to join an existing Lab project (i.e., where Dr. Rose is the lead PI) without the prior agreement of, at a minimum, Dr. Rose and the first author(s).\nThe Lab follows the ICMJE recommendations regarding who is included as an author.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Publications</span>"
    ]
  },
  {
    "objectID": "chapters/07-publications.html#preprints",
    "href": "chapters/07-publications.html#preprints",
    "title": "7  Publications",
    "section": "7.2 Preprints",
    "text": "7.2 Preprints\nWe typically post preprints on arXiv, medRxiv, or as an NBER Working Paper.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Publications</span>"
    ]
  },
  {
    "objectID": "chapters/07-publications.html#on-least-publishable-units",
    "href": "chapters/07-publications.html#on-least-publishable-units",
    "title": "7  Publications",
    "section": "7.3 On Least Publishable Units",
    "text": "7.3 On Least Publishable Units\nWe aim to write papers with an appropriate scope suitable for the goals of the project. We do not focus on salami slicing or least publishable unit papers. This is not to confuse least publishable units with short, rigorous papers. Short papers can be good and high quality with strong standards.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Publications</span>"
    ]
  },
  {
    "objectID": "chapters/07-publications.html#types-of-papers",
    "href": "chapters/07-publications.html#types-of-papers",
    "title": "7  Publications",
    "section": "7.4 Types of Papers",
    "text": "7.4 Types of Papers\nThe structure and content of a manuscript varies by discipline and audience.\nExample Lab publications by journal type: statistics, medical, health policy, health services research, health economics, epidemiology, computer science conference paper.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Publications</span>"
    ]
  },
  {
    "objectID": "chapters/07-publications.html#journals",
    "href": "chapters/07-publications.html#journals",
    "title": "7  Publications",
    "section": "7.5 Journals",
    "text": "7.5 Journals\nThe journal lists below are not exhaustive and largely focus on outlets where Lab members have previously published their work. Decisions about where to submit manuscripts for publication are made collaboratively to balance the needs and priorities of the team with as much deference as possible to what is best for the trainee author(s). We also publish in peer-reviewed conference proceedings, see Section 6.2.\nLab members can view our internal list of upcoming and recent publications here.\n\nHealth Economics & Policy\nJournal of Health Economics, American Journal of Health Economics, Health Affairs, Health Services Research, Medical Care, Medical Decision Making, JAMA Health Forum\n\n\nEpidemiology & Public Health\nAmerican Journal of Epidemiology, Epidemiology, International Journal of Epidemiology, American Journal of Public Health\n\n\nStatistics\nJournal of the American Statistical Association, Biometrics, Biostatistics (COI: Sherri was Co-Editor-in-Chief), Statistics in Medicine, Statistical Methods in Medical Research\n\n\nClinical\nNEJM, JAMA, JAMA Internal Medicine, JAMA Psychiatry\n\n\nHealth Informatics & Digital Health\nJAMIA, BMJ Health & Care Informatics, Lancet Digital Health",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Publications</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html",
    "href": "chapters/08-code-review.html",
    "title": "8  Code Review",
    "section": "",
    "text": "8.1 Why Code Review?\nWe define code review as a process by which a second person reviews code before it is merged into the main branch of a repository. If that sounds like software engineering to you, you may wonder if code review is suitable for science. Code review is a critical part of the scientific process. It is a way to ensure that code is reproducible and that it is understandable and maintainable by others. Code review is also a way to share knowledge and learn from others.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#why-code-review",
    "href": "chapters/08-code-review.html#why-code-review",
    "title": "8  Code Review",
    "section": "",
    "text": "8.1.1 Code Review Makes Better Science\nIn software engineering, code review emerged as programmers realized it was much easier and cheaper to fix bugs before a product reached the user. In science, code review is a way to catch bugs before your research reaches the public. Bugs, from a scientific point of view, have the same meaning as in software engineering (i.e., a mistake in the code) and a broader meaning (i.e., an error in the analysis). Code review can help catch both types of bugs.\nCode review is also a way to ensure that code is reproducible. A second person who can run your code is a good reproducibility test: it simulates what would happen when someone else, much further removed from the project, tries to run your code. Working with the reviewer to make the code run for them will produce more robust code.\nFinally, code review is a way to ensure that code is understandable and maintainable by others. When you are deep into a project, you often are so in tune with the work that very complex things can seem obvious. A second person can help you identify places where this may not be so. A bit of back and forth between the author and the reviewer is natural, as the author explains their code and the reviewer asks questions. When you treat this exchange as a continuous improvement process, you’ll find that your code’s ability to stand on its own improves.\nAll three of these goals help your most important collaborator: future you, who has no idea what current you was thinking when you wrote that code. No, really, you will forget.\n\n\n8.1.2 Code Review Makes Better Teams\nCode review is inherently collaborative. At times, the life of a researcher can feel isolated and lonely. Code review brings someone else into your work and your way of thinking. That relationship is fertile ground for learning and growth.\nReviewing code is a virtuous cycle of education. More experienced coders can help less experienced ones write more idiomatic code. More experienced scientists can improve the use of specific methodology and other types of domain knowledge. Less experienced coders can help more experienced users learn new paradigms in coding. They also serve as a test that code is understandable. Because of this cycle, junior and senior members of the Lab should participate as both reviewers and reviewees.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#code-review-guidelines",
    "href": "chapters/08-code-review.html#code-review-guidelines",
    "title": "8  Code Review",
    "section": "8.2 Code Review Guidelines",
    "text": "8.2 Code Review Guidelines\nNote: These guidelines are adapted from Google’s code review guidelines. This document expands the ideas in Google’s process to the research setting. Where possible, we cite Google’s original guidelines using block quotes:\n\nLike this.\n\nGoogle uses its own tools for managing code and code review, so sometimes you’ll see terms that are a little different. A big one is “CL,” changelist. A CL is Google’s equivalent of a pull request (PR).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#when-to-stop-for-a-code-review",
    "href": "chapters/08-code-review.html#when-to-stop-for-a-code-review",
    "title": "8  Code Review",
    "section": "8.3 When to Stop for a Code Review",
    "text": "8.3 When to Stop for a Code Review\nIn software engineering, code review is a part of daily life. In science, we don’t always write code every day. Some members of the Lab may go weeks or months without writing code. Finishing an analysis before stopping for a code review is also tempting. In other words, adapting the code review process to science requires some flexibility.\nRemember that code review is a continuous improvement process. That means early and often is better. But what does that mean for a research project?\n\n8.3.1 Code Review Gates\nIn the HPDS Lab, we stop at several vital checkpoints or gates for code review. For a traditional research project, these gates are:\n\nInitial data pull (e.g., a SQL query or API call)\nDescriptive tables and figures\nAlgorithms and related tables\nReporting\n\nYour project might not fit this template exactly. For example, if you also have a simulation component to your project, you’ll probably want to stop at several points to review the simulation code. Or, you may have no algorithms in the analysis but a more extensive descriptive analysis. In this case, reviewing the descriptive work all at once is probably too large of a code review to be effective.\nThe general principle is to decide when you absolutely must stop for a code review. Make as many or as few as feels right between them, but always stop at the gates.\n\n\n8.3.2 That Said, Prefer Smaller Changes Per Review\nSmaller changes are more effective and more satisfying to reviewers and reviewees because they are easier to understand and quicker to review.\nHere are some relevant points from Google’s guidelines:\n\nSmall, simple CLs are:\n\nReviewed more quickly. It’s easier for a reviewer to find five minutes several times to review small CLs than to set aside a 30 minute block to review one large CL.\nReviewed more thoroughly. With large changes, reviewers and authors tend to get frustrated by large volumes of detailed commentary shifting back and forth—sometimes to the point where important points get missed or dropped.\nLess likely to introduce bugs. Since you’re making fewer changes, it’s easier for you and your reviewer to reason effectively about the impact of the CL and see if a bug has been introduced. …\nEasier to merge. Working on a large CL takes a long time, so you will have lots of conflicts when you merge, and you will have to merge frequently.\nEasier to design well. It’s a lot easier to polish the design and code health of a small change than it is to refine all the details of a large change.\nLess blocking on reviews. Sending self-contained portions of your overall change allows you to continue coding while you wait for your current CL in review. …\n\n\nThere are three good ways to think about the size of your PR: number of lines changed, numbers of files changed, and the scope of the changes.\nIn software engineering, 100-500 lines of code are often considered the optimal PR size. For research, it depends on the nature of the code. For some types of data cleaning, you may need more lines than this. For some algorithmic problems, you may want to step well before this number of lines. Each review should be digestible for the reviewer. You’ll get a feel for what is too big as you do more code reviews.\nSimilarly, you should limit the number of files changed or added where reasonable. Sometimes, a change to your project automatically generates many files, e.g., figures for a report. That’s okay. But breaking up the review into smaller pieces is a good idea if you are making changes to many unrelated files.\nFinally, keep your changes focused on one area.\n\nIn general, the right size for a CL is one self-contained change.\n\nYou should be able to describe what the change is succinctly. It’s also okay for such changes to touch different parts of your project. For instance, when it comes time for estimation, you may need to modify your data cleaning code for the process to work. But if you then decide to make unrelated changes to an earlier figure, handle that in a separate PR and review.\nAs a note, “too small” is rarely a problem in code review. Lean towards smaller reviews.\n\n\n8.3.3 Code Review Is Progress, Too\nOften, it can be challenging to stop and wait on your project. For researchers in particular, where typically a single person leads an analysis, you may feel like you are the only one who can do the work. But remember that code review is part of the work. It’s not a delay in the work. It is the work.\nCode review is progress. Take a break and enjoy your project moving forward in peace!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#pre-review-checklist",
    "href": "chapters/08-code-review.html#pre-review-checklist",
    "title": "8  Code Review",
    "section": "8.4 Pre-Review Checklist",
    "text": "8.4 Pre-Review Checklist\n\nYour code review should meet Lab standards. Before requesting a review, make sure you have done the following:\n\nPrefer making a change as a pull request. PRs are more straightforward to review than code on the main branch because they are isolated from other changes. GitHub also has tools to help you review code as a PR.\nRe-run your code from a blank slate environment. This means running your code in an entirely fresh session so that you know your code matches your results.\nFollow the code style guide for the language you are using. Prefer an automatic tool to style your code for you.\nMake sure it is clear how to run the code. This might be related to where to run the code (e.g., GCP) or how to install the project dependencies.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#requesting-a-code-review",
    "href": "chapters/08-code-review.html#requesting-a-code-review",
    "title": "8  Code Review",
    "section": "8.5 Requesting a Code Review",
    "text": "8.5 Requesting a Code Review\n\n8.5.1 Identifying Reviewers\nGoogle has a process called readability reviews. In short, someone with readability credentials is an expert on how Google uses a particular language. Readability helps maintain a consistent style and propagate idiomatic code for a given language. Google also requires reviews from another person familiar with the project and an “owner” of the code.\nWhen identifying a reviewer, use this framework. Who can review your code from the perspective of the language you are using? Who can review your code from the perspective of the domain you are working in? Who can review your code from a methodological standpoint? Usually, you are the owner of your research project, but sometimes, you are contributing to someone else’s code. In that case, make sure they are part of the review process.\nFor sensitive data, prefer to have a reviewer who has access to the data so they can run the code. See Chapter 4 for details on requirements to access project data. Code reviewers who are not on the associated IRB or cleared for data access cannot run the code directly on the data.\nNote: You are probably the person most familiar with your project. That’s okay. The reviewer is there to help you make the code better, so they don’t necessarily need to be an expert.\nIdeally, you should pick only one or two reviewers because many can slow down the process. The best way to do this is to identify someone who meets as many of the requirements for your review as possible, e.g., who can help improve both your code and the methodology. If you can’t find someone who meets your requirements, you can ask more than one person to review your code.\nIf you’re not sure who to ask, start with Malcolm. (Typically, Malcolm is on every IRB and can access data directly.)\nIt may be that your first choice reviewer is not available to do the review promptly. It’s better to have someone else review the code, although it’s a judgment call. Occasionally, you should wait for a particular person to be available if they serve as a critical reviewer. Talk openly about timelines and expectations with your reviewer.\n\n\n8.5.2 Writing a Good Description\nThe description of your code review should be a summary of what you are trying to accomplish.\n\nThe first line should be a short, focused summary, while the rest of the description should fill in the details and include any supplemental information a reader needs to understand the changelist holistically. It might include a brief description of the problem that’s being solved and why this is the best approach. If there are any shortcomings to the approach, they should be mentioned. If relevant, include background information such as bug numbers, benchmark results, and links to design documents.\n\nSee Google’s documentation for examples of good and bad descriptions.\nYou may also want to guide reviewers on where to focus their attention.\nPolish the description before requesting a review and before merging.\n\nCLs can undergo significant change during review. It can be worthwhile to review a CL description before submitting the CL, to ensure that the description still reflects what the CL does.\n\n\n\n8.5.3 Iterating on Code Review\n\n8.5.3.1 Responding to Reviewer Comments\nWhen you receive comments from a reviewer, you should make a change or respond to them. If you don’t understand a comment, ask for clarification. If you disagree with a comment, explain why and discuss it. Occasionally, you may want a third party to help decide on a change.\nBe respectful and expect the same from your reviewer. Our goal is to improve each other’s work. Remember that, much like a peer review on a manuscript, the review is about your work, not you. Nevertheless, receiving feedback on a project you’ve worked hard on can be difficult. If you are frustrated, take a break and return to it later. Assume good intent from your reviewer.\n\nRemember, courtesy and respect should always be a first priority.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#giving-a-code-review",
    "href": "chapters/08-code-review.html#giving-a-code-review",
    "title": "8  Code Review",
    "section": "8.6 Giving a Code Review",
    "text": "8.6 Giving a Code Review\nThe first and most important rule of code review is that you are there to improve the code, not perfect it. If the code review ends with the code in a better state than it started, you have succeeded.\n\n8.6.1 What to Look for in a Code Review\nThe key things we look for in a code review are correctness, readability, and reproducibility. Keep your eye out for bugs, where the code is hard to understand, and where the code is hard to run.\nSome other relevant points from the Google documentation are below.\nApproaching a code review:\n\n\nDoes the change make sense? Does it have a good description?\nLook at the most important part of the change first. Is it well-designed overall?\nLook at the rest of the CL in an appropriate sequence.\n\n\nDetailed points to review:\n\nIn the general case, look at every line of code that you have been assigned to review. Some things like data files, generated code, or large data structures you can scan over sometimes, but don’t scan over a human-written class, function, or block of code and assume that what’s inside of it is okay. Obviously some code deserves more careful scrutiny than other code—that’s a judgment call that you have to make—but you should at least be sure that you understand what all the code is doing. If it’s too hard for you to read the code and this is slowing down the review, then you should let the developer know that and wait for them to clarify it before you try to review it. At Google, we hire great software engineers, and you are one of them. If you can’t understand the code, it’s very likely that other developers won’t either. So you’re also helping future developers understand this code, when you ask the developer to clarify it.\n\n\nWhat if it doesn’t make sense for you to review every line? … In these cases, note in a comment which parts you reviewed.\n\nPoints to consider:\n\n\nThe code is well-designed. …\nAny parallel programming is done safely.\nThe code isn’t more complex than it needs to be.\nThe developer isn’t implementing things they might need in the future but don’t know they need now. …\nThe developer used clear names for everything.\nComments are clear and useful, and mostly explain why instead of what.\nCode is appropriately documented […]\nThe code conforms to our style guides.\n\n\nUse GitHub’s comments and suggestion features to speed up the process and put discussions close to the code.\n\n\n8.6.2 What to Avoid\nA fine line exists between making code more idiomatic and making it your code. Avoid the latter. Just because you would do something differently doesn’t mean it’s wrong.\nAdditionally, you should avoid making changes unrelated to the code review. File a new issue to address in a separate change.\nFinally, there is a topic that is usually inappropriate for code review: design of the analysis. Not only is a code review a poor fit for such a discussion, but it also hampers the rigor of the study, e.g., adding in ad-hoc analyses that deviate from the research plan. If an analysis has a plan or pre-registration, it is appropriate to discuss if something in the code is done differently from the plan.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#timeline-and-speed-of-review",
    "href": "chapters/08-code-review.html#timeline-and-speed-of-review",
    "title": "8  Code Review",
    "section": "8.7 Timeline and Speed of Review",
    "text": "8.7 Timeline and Speed of Review\nDiscuss your timeline for review with the author. If you cannot review the code promptly, let the author know. Fast code review is essential to maintain both momentum on a project and satisfaction with the review process.\nGoogle’s entire guide on the speed of reviews is excellent. Here are some important points:\n\nPrioritize reviews but not at the expense of your own focus time. Try to get to it when you have a break in your focused work.\nGoogle recommends a single business day as a turnaround time. This goal is good, but it’s not always possible in a research setting with competing priorities. Nevertheless, make code review an important part of your workday. Discuss the timeline for review with the author. Your pending review should not be blocking their progress.\n\n\n8.7.1 LGTM\nThe traditional stamp of approval is “LGTM” – “looks good to me!” However you say it, make a clear statement of approval, even if only through GitHub’s approval mechanism.\n\n\n8.7.2 Giving Comments and Responding to Reviewee Comments\nPer Google’s How to write code review comments:\n\n\nBe kind.\nExplain your reasoning.\nBalance giving explicit directions with just pointing out problems and letting the developer decide.\nEncourage developers to simplify code or add code comments instead of just explaining the complexity to you.\n\n\nAlso, clarify when something is a suggestion or for the author’s information. If you have a suggestion that is out of scope for the review, file a separate issue.\nThere is also some evidence that women and racial minorities are more likely to experience pushback in code review. Be mindful that your review is helpful and not harmful.\nSometimes, authors may disagree with you.\n\nWhen a developer disagrees with your suggestion, first take a moment to consider if they are correct. Often, they are closer to the code than you are, and so they might really have a better insight about certain aspects of it. Does their argument make sense? Does it make sense from a code health perspective? If so, let them know that they are right and let the issue drop.\n\nIn research, too, the author is often much closer to the code and project than you. Nevertheless, your job is to help the author improve their code, so it’s okay to (politely) advocate for a change you think is important. If you can’t agree, ask a third party to help.\n\nRemember, courtesy and respect should always be a first priority\n\nBy the way, remember to point out things you like!\n\nIf you see something nice in the CL, tell the developer, especially when they addressed one of your comments in a great way. Code reviews often just focus on mistakes, but they should offer encouragement and appreciation for good practices, as well. It’s sometimes even more valuable, in terms of mentoring, to tell a developer what they did right than to tell them what they did wrong.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/08-code-review.html#appendix-code-review-tools-on-github",
    "href": "chapters/08-code-review.html#appendix-code-review-tools-on-github",
    "title": "8  Code Review",
    "section": "8.8 Appendix: Code Review Tools on GitHub",
    "text": "8.8 Appendix: Code Review Tools on GitHub\nGitHub offers top-notch tools for code review. Use a pull request workflow to manage your code changes to take advantage of them. Here are some recommended articles on tools for code review:\n\nReviewing pull requests\nCommenting on changes and making suggested changes\nAccepting changes\nSometimes, a PR results in a merge conflict. GitHub has a GUI for handling basic conflicts. You might also like something like GitKraken to help you manage more complex conflicts.\nMerging a pull request",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Code Review</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html",
    "href": "chapters/09-code-workflow-agreements.html",
    "title": "9  Code Workflow Team Agreements",
    "section": "",
    "text": "9.1 Team Agreements\nThis chapter documents the code workflow team agreements for our Lab. By team agreements, we mean a set of coding principles and workflows we have all agreed to adhere to. This is a living document: we can update as the team and coding ecosystems also change over time. If you have concerns or suggestions about our workflows, feel free to propose alternatives for the team to consider collectively.\nWe focus on workflows that allow individual members freedom to code how they prefer while maintaining reproducibility and ease of collaboration. Thus, we don’t tell you which language to use or, for the most part, which tools to use within a given language (unless there is a key tool we use to maintain the agreements).\nCurrently, most of our team uses R, Python, or both. Thus, we offer language-specific guidance for those two languages. We’ll add relevant recommendations if other languages become more prominent in the team.\nThroughout this document, we’ll organize language-specific guidance in tab sets like this:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#team-agreements",
    "href": "chapters/09-code-workflow-agreements.html#team-agreements",
    "title": "9  Code Workflow Team Agreements",
    "section": "",
    "text": "RPython\n\n\nThis is how we do things in R.\n\n\nThis is how we do things in Python.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nContact Malcolm for support if you want help setting up or using any guidelines in this document.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#sec-code-style",
    "href": "chapters/09-code-workflow-agreements.html#sec-code-style",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.2 Code Style",
    "text": "9.2 Code Style\nConsistency of code style is essential for collaboration. It speeds up the reading of code and reduces debates on what code should look like. We prefer automated styling tools such that you can write however you like and quickly follow the team style guide by running the tool.\n\nRPython\n\n\nR code should follow the Tidyverse style guide, automatically styled by the styler package. Despite the name, this guide also applies to non-Tidyverse code, including base R and data.table.\n# style all R-related files in the current directory\nstyler::style_dir()\nNote that you should run styler interactively in the console, not in one of your R scripts.\nStyler also has a helpful add-in for RStudio to let you style the active file, a code selection, etc.\n\n\nFor Python, we use the Black style guide. Black comes with its own styler, but Ruff implements it more quickly and flexibly, so that’s our recommendation. Either tool is fine in practice, though, as it should result in the same styling.\nRuff is installable through pip:\npip install ruff\nRun this command in the terminal to format the Python code in your directory:\nruff format\nNote: Quarto files are not widely supported yet. See this issue for ruff: https://github.com/astral-sh/ruff/issues/6140.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe do not currently require a linter, but they can be handy to analyze your code for areas of improvement.\n\nRPython\n\n\nIn R, use lintr. Linting works particularly well in RStudio, which will tag lines of code with specific suggestions.\nlintr::lint_dir()\n\n\nIn Python, use Ruff’s linter.\nruff check \nYou can also automatically fix any suggestions or re-lint on changes.\n# lint and fix\nruff check --fix    \n# re-lint when files change\nruff check --watch",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#sec-naming",
    "href": "chapters/09-code-workflow-agreements.html#sec-naming",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.3 Naming Things",
    "text": "9.3 Naming Things\nNaming things is an aspect of code style, but it merits a little extra attention because it is important to readability. In general, strive to make your code self-documenting, e.g., someone else should be able to read your code and understand what it’s doing without talking to you about it. Self-documenting is easier said than done, so code clearness is an essential type of feedback in code review.\nUse descriptive names for objects in code, e.g., model_results rather than out or kangaroo_data rather than k.\n\n\n\n\n\n\nTip\n\n\n\nIt’s OK to use terse names where the meaning is generally understood, like n for counts or i and j for loop counters.\n\n\nSimilarly, for functions, describe what the function is doing, preferably in verb format, e.g., simulate_data() is better than f() or data_simulator()\nFor both R and Python, prefer snake_case over camelCase and other naming style conventions unless otherwise appropriate, e.g., the name of a class for object-oriented programming in Python.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#project-organization",
    "href": "chapters/09-code-workflow-agreements.html#project-organization",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.4 Project Organization",
    "text": "9.4 Project Organization\nWe try to be flexible about project organization, as being too prescriptive often backfires. The rule of thumb is to try to make it obvious where everything is by using descriptive folder and file names, e.g., R/ or scripts/ to organize source code and output or reports to organize research outputs. You should also include guidance on the organization of your project in a README (see Section 9.7.2).\nFor instance, an R project might look like this:\nR/\nfigures/\nreports/\nmy_project.Rproj\nor like this:\nsql/\nfunctions/\nrun.R\nmy_project.Rproj\nBoth setups are understandable and reasonable; the vital part is that users know where to find important files and how to use them.\nThere are a few tools that will help you set up a project.\n\n\n\n\n\n\nTip\n\n\n\nRStudio supports the idea of RStudio Projects. These are handy setups because they set your working directory wherever your project is on your computer. See Project-Oriented Workflows for more.\nUse the RStudio interface to create a new project or create one in the console with usethis::create_project(\"/path/to/your/new/project\").\n\n\nWe also have a couple of essential guidelines for what is in a project.\n\n9.4.1 Don’t Include Sensitive Information on Git and GitHub\n.gitignore any sensitive data. For instance, your .gitignore file should contain lines for data/ and data-raw/ if those two folders contain sensitive data. Another common source of information leaks is cached data, such as Jupyter checkpoints or a knitr cache.\n\n\n\n\n\n\nTip\n\n\n\nFor R users, consider running usethis::git_vaccinate() in the console. Running this command will add standard R-related files that shouldn’t be in version control to your global gitignore. It won’t protect you from including sensitive data, but it serves as a backup configuration for files that usually don’t need to be included.\nusethis also has a function called use_git_ignore() for adding files and folders to .gitignore, which might feel more natural to the R workflow.\n\n\nA project that has sensitive data and uses Jupyter might have a .gitignore file in the root directory that looks like this:\ndata/\n.ipynb_checkpoints\n*/__pycache__\nHere, data/ is ignored because it contains sensitive data, .ipynb_checkpoints is ignored because it might accidentally have such data, and __pycache__ is ignored not because it is a data cache (despite its name) but that it’s a commonly ignored development output in Python projects.\n\n\n\n\n\n\nTip\n\n\n\nSee GitHub’s collection of commonly gitignored files and folders organized by programming language for more suggestions on files you might want to exclude from your repository. Not all these are related to sensitive information; some are output objects considered unnecessary or cluttering.\n\n\n\n\n9.4.2 Don’t Include Absolute Directories That Are Not Accessible to Others\nA common mistake in code is to hardcode a path that only exists on your computer, e.g.\ndf = pd.read_csv(\"/users/malcolmbarrett/Downloads/data.csv\")\nPrefer file paths that are relative to the root directory of your project. data.csv should be in my project directory, which I can access with relative paths:\ndf = pd.read_csv(\"data/data.csv\")\n\n\n\n\n\n\nTip\n\n\n\nFor R users, the here package can let you refer to files and folders from the root directory of your project no matter where the code exists. here can be helpful when, for instance, you have a report in a reports/ folder but want to refer to a data file in the data/ folder. Instead of backtracking ../data/data.csv, you can write here(\"data\", \"data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#sec-quarto",
    "href": "chapters/09-code-workflow-agreements.html#sec-quarto",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.5 Generate Reproducible Documents with Quarto",
    "text": "9.5 Generate Reproducible Documents with Quarto\nPrefer using Quarto for documents, particularly those that generate a research product like a journal article. Quarto is a scientific publishing framework that weaves Markdown-based text with computational results from R, Python, Julia, etc. You can also mix languages in a single document.\nYou can also use Quarto interactively in RStudio and VS Code much as you can with R Markdown or Jupyter notebooks.\nThe documentation on the Quarto website is excellent, so check that for guidance on the latest features. A handy collection of pages for research is the Scholarly Writing section of the Authoring guide.\nFor generating PDF documents with Quarto, you’ll need a TeX distribution. We recommend installing TinyTex, which is a good general-purpose, lightweight distribution.\nSee our trainings repository for past Lab trainings on Quarto.\n\nRPython\n\n\nRStudio comes with an installation of Quarto, so you likely don’t need to install it. However, if you install a newer version of Quarto, RStudio will use that automatically, making it reasonably seamless to upgrade.\nIf you prefer, you can also use R and Quarto in VS Code.\nPrefer Quarto over R Markdown; Quarto supersedes R Markdown and all significant improvements and developments will be there instead of R Markdown.\nFor rendering PDF documents, you can install TinyTex with the tinytex package:\ntinytex::install_tinytex()\n\n\nPrefer Quarto .qmd files over Jupyter files; Quarto can run interactively like a notebook but has better reproducibility and version control properties. That said, you can use both using Quarto to render Jupyter files. Quarto also has tooling for switching back and forth between formats.\nThe best way to use Quarto for Python projects is VS Code using the Quarto extension. This extension allows you to render files easily while also running code interactively. RStudio also supports Python projects, so it’s another good option if you also use R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#use-git-and-github",
    "href": "chapters/09-code-workflow-agreements.html#use-git-and-github",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.6 Use Git and GitHub",
    "text": "9.6 Use Git and GitHub\nYou should manage your project with version control and hosted for collaborators to access. We use Git and GitHub for managing and hosting version-controlled repositories, respectfully. We also have a GitHub organization where we keep team projects.\nSee Chapter 8 for how to manage code changes to your repository.\n\n\n\n\n\n\nTip\n\n\n\nFor R users, there are two helpful functions in usethis to manage your git setup for a project: usethis::use_git() will initialize a git repository and activate the git UI in RStudio; usethis::use_github() will create and link a GitHub repository, as well as make your first push. Note that use_github() has an organisation argument where you can specify a GitHub organization to create the repository under, such as use_github(organisation = \"StanfordHPDS\").\nSee also the PR workflow vignette for making and managing pull requests in R. You might also find Happy Git and GitHub for the useR a helpful perspective.\n\n\n\n9.6.1 Release on GitHub\nGitHub has a mechanism for making formal releases of repositories. This takes a snapshot of the repository at the time of release and attaches a name.\nMaking a “release,” of course, has its origins in software versions. However, it’s also helpful in tracking significant milestones in a project. For instance, make a release upon submission to a journal and another upon publication. That allows you to easily revisit the state of the repository at those two time points. GitHub also makes downloading the repository in releases easy, making it an excellent point to direct people not using Git.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#documentation",
    "href": "chapters/09-code-workflow-agreements.html#documentation",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.7 Documentation",
    "text": "9.7 Documentation\nThe first step in documenting your code is writing clear, readable code. In other words, someone else should be able to read your code and understand what it’s doing. The best way to test this out is a code review: can somebody else understand your code? As discussed in Section 9.3, code that is itself readable is sometimes called self-documenting code. It’s not always possible and shouldn’t be the only type of documentation, but it’s an excellent goal to strive towards.\n\n9.7.1 A Note about Comments\nComments exist in virtually every programming language because programming languages are for humans, not computers. Adding commentary is essential to making it understandable to a person.\nThat said, many comments are of two less helpful varieties: what comments and deodorant comments. “What” comments explain what the code is doing, usually redundantly. This type of comment is unnecessary because it’s clear from the code what is happening:\n# add 1 and 2 together\n1 + 2\nDeodorant comments are added because the code is unclear. This idea comes from what Martin Fowler calls “code smells”, a particular odor in code that suggests a more profound problem or lack of clarity.\n\n…comments aren’t a bad smell; indeed they are a sweet smell. The reason we mention comments here is that comments often are used as a deodorant.\n\nSometimes, blocks of comments explain code that could be refactored to be more straightforward:\n\nWhen you feel the need to write a comment, first try to refactor the code so that any comment becomes superfluous\n\nAgain, code comments are not bad. They are helpful and sometimes downright necessary. Some code is unavoidably complex, so context as to why and what the code is doing makes it more understandable.\nWhen you get feedback in a code review that something is hard to understand, try to clarify the code, then add comments as necessary.\n\n\n9.7.2 README\nA README file is commonly included in a project to explain how to use it or other relevant information. README.md files have special properties on GitHub. These are text files that use markdown syntax. When you visit a repository on GitHub, the README is rendered and serves as a front page for your project. You can manually include a README.md file or render one with a README.Rmd or README.qmd file.\n\n\n\n\n\n\nTip\n\n\n\nFor R users, running usethis::use_readme_md() and usethis::use_readme_rmd() will create README.md and README.Rmd, respectively. You only need to pick one of these.\n\n\nFor our purposes, a README should include at least:\n\nA description of the project, such as an abstract\nA brief explanation of the file structure of the repository\nDetails on how to run the code (Section 9.8) in the project, such as installing dependencies, versions of R or Python for the project, etc.\nAny other necessary information to understand or use the project",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#sec-running-code",
    "href": "chapters/09-code-workflow-agreements.html#sec-running-code",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.8 Running Code",
    "text": "9.8 Running Code\n\n9.8.1 Use a Blank Slate\nYou should regularly run your code in a blank slate—running everything from scratch.\n\nRPython\n\n\nBy default, R stores and loads the interactive environment you use between R sessions. This is a bad default for reproducibility. Teach R not to do this by running this command once in your terminal:\nusethis::use_blank_slate()\nRelatedly, your script should not contain rm(list = ls()). Using a blank slate and restarting your R session is the better approach because it really starts from scratch, whereas rm(list = ls()) only clears objects in the global environment. See Project-Oriented Workflows for a discussion on this.\n\n\nIn Python, the primary way that people end up with environments that are out of sync with their code is by not regularly restarting their Jupyter kernel. Instead, regularly restart the kernel and run all cells to make sure your code still works (and works as you expect).\nNote that this applies only to Jupyter Notebooks. While Quarto uses the Jupyter kernel for running Python code, it always starts a new session when rendering.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nRendering a Quarto document always runs code from scratch by default.\n\n\n\n\n9.8.2 Provide Guidance on How to Run Your Code\nYour README should include guidance on how to run your code. For instance, if there is a command to run the entire project, include information about that process (this is usually related to pipeline-managed code as discussed in the optional Section 9.10.1). If you intend the user to run scripts in a particular order, describe how.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#sec-pkg-env",
    "href": "chapters/09-code-workflow-agreements.html#sec-pkg-env",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.9 Lock your Package Versions",
    "text": "9.9 Lock your Package Versions\nInstalling project dependencies is a part of running code, but its peculiarities for reproducibility merit some attention of its own.\nBoth packages within a language and languages themselves frequently change, making running code over time (as new versions are released) and space (the versions on your computer vs. someone else’s computer) difficult.\nA vital check of this problem is code review. The reviewer serves as a proxy for being able to run your code somewhere else. If it doesn’t work for your reviewer, discuss and document the process to make it so.\nThere are also tools for installing versions of packages local to the project such that a project will always use identical package versions.\n\n\n\n\n\n\nNote\n\n\n\nWe leave it up to the project’s author when they activate a package environment manager. Sometimes, waiting while you iterate through a project, adding and removing dependencies as you go, is helpful. Sometimes, it’s easier to start in a controlled environment.\nAt the very least, your project should end in such a state. The last check for this is pre-submission (Chapter 10).\n\n\nHere are the tools we use for managing software versions.\n\nRPython\n\n\nIn R, use the renv package to manage R package versions. renv is an interactive tool, so you should use it primarily in the console.\nTo initiate a project:\nrenv::init()\nAmong other things, this will create renv.lock, which needs to be included in your git repository.\nCheck a project’s dependency status with\nrenv::status()\nAnd update the packages in your lock file with\nrenv::snapshot()\nWhen you use renv, your project README should explain how to use it, e.g.\n# install.packages(\"renv\")\n# install the dependencies for the project\nrenv::restore()\nOne common problem in renv is that it keeps track of but does not manage the R version. If you need to switch between R versions, consider using rig. Rig is not an R package but a separate software that lets you install and switch between R versions. On servers, RStudio Workbench will let you change versions, as well, provided that they are installed.\n\n\nNote: The Python ecosystem for managing environments is vast. See https://alpopkes.com/posts/python/packaging_tools/ for an overview.\nWe currently recommend Conda via the miniconda distribution. Conda allows you to install packages from Conda channels, set up virtual environments, and control the version of Python.\nNotably, we recommend a Conda-first approach. If you’re installing a package, use conda install before trying pip install. PyPi and Conda channels build packages differently, so it’s best to stick with one style where possible. However, Conda has a smaller, more refined selection of packages compared to PyPi, so some packages may only be available via pip. See this blog post for more information on the differences between the two.\nConda treats Python like other packages, so managing it is similar. One helpful thing you can do is specify a Python version while creating an environment. See the Conda documentation for other ways of interacting with the Python version.\n# create a new virtual environment called projectenv\nconda create --name projectenv python=3.12.1\n\n# activate the environment projectenv\nconda activate projectenv\n\n# add a package to your environment\nconda install polars",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/09-code-workflow-agreements.html#sec-opt-in",
    "href": "chapters/09-code-workflow-agreements.html#sec-opt-in",
    "title": "9  Code Workflow Team Agreements",
    "section": "9.10 Opt-in Workflows",
    "text": "9.10 Opt-in Workflows\nOpt-in workflows are things we do not require for a project but for which we offer guidance. Such workflows also allow the team to experiment with new things and see what works for projects and when.\n\n9.10.1 Pipelines\nPipeline tools are software that manage the execution of code. What’s practical about this for research projects is that pipeline tools track the relationship between components in your project (meaning it knows which order to run things in automatically) and will only run those components when they are out of date (meaning you don’t necessarily need to rerun your entire project because you updated one part of the code).\nPipeline tools are helpful for projects of any size, but they are particularly suited to complex or computationally intense projects.\n\nRPython\n\n\nThe best pipeline tool in R is the targets package. targets is a native R tool, making it easy to work with R objects. It works particularly well with Quarto and R Markdown, allowing you to reduce the amount of code in a report while managing it reproducibly.\ntargets has excellent documentation and tutorials, so we point you there for guidance.\nIt’s also possible to use tools like Make and Snakemake, among others (see the Python tab), with R, although we recommend targets for projects that are mostly R.\n\n\nPython has several pipeline tools that are used in data engineering. For these larger data projects, these tools are sometimes called orchestration tools. That said, many of them are much more complex than is needed for a single research project.\nWe don’t currently have a recommendation. Here are the tools we should explore:\n\nMake: Make is one of the oldest and most popular pipeline tools–over 40 years old. It shows its age in some ways, but it’s also battle-tested. See this tutorial for an example of running an analysis with Make.\nSnakemake: A Python tool influenced by Make, it’s very similar in spirit but more modern. It’s easier to read and customize, and you can write Python code within the Snakefile. It also works nicely with Python and R scripts. One handy thing is that you can access Snakemake inputs and outputs through magic objects for both Python and R. That makes it useful for, e.g., dynamic rules and rules where you want to use inputs for reports. Snakemake has good support for Conda environments in particular.\nDagster: Dagster is a different approach that uses decorators to tag Python functions as “assets,” the apparent equivalent of targets in other tools. It has a nice UI tool for visualizing the nodes and relationships of the project, as well as “materializing” (running) them. Dagster is a company, but the tool has an open-source version.\nPrefect: Prefect is an increasingly popular tool for orchestration. Like Dagster, it is a company-based open-source tool that uses decorators to tag Python functions. It also has a UI.\nAirFlow: AirFlow is an open-source orchestration tool. Notably, Google Cloud supports a UI for creating and running AirFlow called Cloud Composer. AirFlow also has a recent decorator-based API called TaskFlow, which is more in line with some of the above tools.\n\n\n\n\n\n\n9.10.2 Testing\nIn scientific work, two types of code tests are useful: code expectations and data expectations. Code should behave the way you expect, and data should exist the way you expect. If that is not the case, you either have identified a problem with your code and data or a problem with your expectations.\nAutomating such tests in code expectations is helpful because you can run them whenever you have code or data changes. Moreover, you’re probably already probing these expectations informally in some form or another.\n\nRPython\n\n\nYou can write code tests using simple tools in base R like stop() and stopifnot(), but the most robust suite for testing code is the testthat package. While testthat is designed for R packages, it’s also useful for testing functions in your analysis code. In addition to the testthat documentation, see the testing chapter of R Packages for more.\nThere are several tools for testing data. Again, you can use simple logical statements with stop() and friends, but the most comprehensive tool in R for data quality checks is pointblank.\nIn RStudio, you can automate tests even when you’re not in a package by activating “package mode.” All this means is adding a DESCRIPTION file. To do this, run usethis::use_description(check_name = FALSE) and restart RStudio. Then, you can run any tests in the test/ folder via the Build pane.\n\n\nYou can write code tests using simple tools in Python like assert, but there are also many libraries for formal testing. The most popular is unittest.\nThere are several tools for testing data. Again, you can use simple logical statements with assert and friends, but the most comprehensive tool in Python for data quality checks is Great Expectations.\n\n\n\n\n\n9.10.3 Docker\nDocker is a tool for reproducing not just the software and packages used in an analysis but the entire computational environment. As it’s got more of a learning curve, we do not currently require it for projects. However, consider it for projects where reproducibility is crucial or where you want to feel more confident that the code will be runable for many years.\nDocker is a general tool that works with R, Python, and just about anything else you like to use.\nHere’s a Docker tutorial for data analysis: https://github.com/RamiKrispin/sdsu-docker-workshop.\nIf interest in Docker grows on the team, we’ll also explore creating support for Docker workflows, such as prebuilt images and Dockerfiles the team can use.\n\nRPython\n\n\nDocker doesn’t require a different approach for R, but many existing images save time. Here is an overview of running R code in Docker: https://raps-with-r.dev/repro_cont.html.\n\n\nDocker doesn’t require a different approach for Python, but some Python-specific tools will save time. Here’s a tutorial on setting up and using Docker with Python in VS Code: https://github.com/RamiKrispin/vscode-python",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Code Workflow Team Agreements</span>"
    ]
  },
  {
    "objectID": "chapters/10-pre-flight-checklist.html",
    "href": "chapters/10-pre-flight-checklist.html",
    "title": "10  Pre-Flight Checklist for Publications",
    "section": "",
    "text": "10.1 Crosscheck\nAs discussed in Chapter 8 and Chapter 9, your analysis should be regularly checked for correctness and adherence to Lab agreements, not just at the end of the project. However, it’s useful to have a final check (by you and at least one other person) before submitting for publication. Regular code review will make this final check quicker and easier, so please do both.\nNote: For pre-prints, run through this checklist for both uploading to the pre-print server as well as for when you formally submit the project to a journal.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pre-Flight Checklist for Publications</span>"
    ]
  },
  {
    "objectID": "chapters/10-pre-flight-checklist.html#pre-flight-checklist",
    "href": "chapters/10-pre-flight-checklist.html#pre-flight-checklist",
    "title": "10  Pre-Flight Checklist for Publications",
    "section": "10.2 Pre-Flight Checklist",
    "text": "10.2 Pre-Flight Checklist\n\n10.2.1 Repository and Files\n\nConfirm again that no sensitive data is included in the repository. Check files, Jupyter caches, etc.\nThere is a README file (any of: README.md, README.Rmd, README.qmd).\nREADME contains clear instructions on how to run the code. See Section 9.7.2 for details.\nCreate a new public repository for the project and archive the private repository.\n\n\n\n10.2.2 Code and Reproducibility\n\nThe code follows the team style guide. See Section 9.2 for details.\nQuery code from user interfaces, e.g., Redivis or BigQuery, are up to date in version control.\nThe code uses a package environment to lock packages. See Section 9.9 for details.\nManuscript is generatable via Quarto. See Section 9.5 for details.\nRun the code one last time in a fresh environment, preferably on a different computer. See Section 9.8 for details.\n\n\n\n10.2.3 Submission\n\nFollow Data Use Agreement requirements regarding project review from a third party prior to submission.\nCheck if journal has specific code and reproducibility guidelines.\nCheck figures and tables for correctness.\nCheck https://www.zotero.org/styles for a citation style file for the journal of interest. Include the file in your Quarto metadata and in the repository.\nFor sensitive data, confirm small cell definition for the project. Check that no small cells are included in tables or figures. See for Chapter 4 for details.\nUse a GitHub release for the submission version. See Section 9.6.1 for details.\nIf there are any research outputs other than a manuscript, such as a (simulated/synthetic) data, register them for a DOI at a service like OSF or Zenodo.\n\n\n\n10.2.4 Acceptance\n\nIf the project exists as a pre-print, update the pre-print page to include information about the peer-reviewed publication.\n\n\n\n10.2.5 Opt-in Workflows (Section 9.10)\n\nOpt-in: Pipeline code is documented and with clear instructions for running the pipeline. See Section 9.10.1 for details.\nOpt-in: If code contains tests, tests pass. See Section 9.10.2 for details.\nOpt-in: Dockerfile is included in repository with clear instructions for running the image. See Section 9.10.3 for details.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Pre-Flight Checklist for Publications</span>"
    ]
  }
]